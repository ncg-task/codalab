<p>An NLPContributionGraph submission will be considered complete with predictions made for all 3 tasks (Sentence, Phrases, Triples). 
The evaluation metrics that will be applied are:</p>
<ul>
<li>Sentences: precision, recall and F1</li>
<li>Scientific Term and Predicate Phrases: precision, recall and F1</li>
<li>Triples: precision, recall and F1 overall and for each information unit</li>
</ul>
<p>The focus of NLPContributionGraph is on the structuring of contributions in NLP scholarly articles to form a knowledge graph. 
To allow a thorough evaluation of systems, NLPContributionGraph will have multiple evaluation phases:</p>
<ol>
<li>Practice phase:
<ul>
<li>Training and dev data:
<ul>
<li>contribution sentences from NLP scholarly articles across different IE tasks (e.g., machine translation, named entity recognition, etc.)</li>
<li>phrases from the annotated contribution sentences</li>
<li>triples from the entities under any of the twelve information units listed below</li>
<ul>
<li><font style="font-variant: small-caps">ResearchProblem</font>, 
<font style="font-variant: small-caps">Approach</font>, <font style="font-variant: small-caps">Model</font>, 
<font style="font-variant: small-caps">Code</font>, <font style="font-variant: small-caps">Dataset</font>, 
<font style="font-variant: small-caps">ExperimentalSetup</font>, <font style="font-variant: small-caps">Hyperparameters</font>, 
<font style="font-variant: small-caps">Baselines</font>, <font style="font-variant: small-caps">Results</font>, 
<font style="font-variant: small-caps">Tasks</font>, <font style="font-variant: small-caps">Experiments</font>, 
and <font style="font-variant: small-caps">AblationAnalysis</font></li>
</ul>
</ul>
</li>
<li>Test data:
<ul>
<li>In the pratice phase, the test data is the dev set itself with a different set of annotated articles from those chosen as training data</li>
</ul>
</li>
</ul>
</li>
<li><code>Evaluation Phase 1: End-to-end pipeline testing phase</code>
<ul>
<li>Training and dev data:</li>
<ul>
<li>Same as in the practice phase; no new data will be released</li>
</ul>
<li>Test data:
<ul>
<li>Participants will be provided with unannotated articles that are not part of the training or dev set</li>
<li>The server will host the reference annotations for these articles for participant automated system output evaluations</li>
</ul>
</li>
</ul>
</li>
<li><code>Evaluation Phase 2: Phrases and Triples extraction testing phase</code><br /> 
This phase will have two parts: In <code>Part 1: Phrase Extraction Testing</code>, the participant systems will be given the gold-annotated contribution sentences 
and will be expected to provide purely their scientific term and predicate phrase extraction output; 
In <code>Part 2: Triples Extraction Testing</code>, the participant systems will be given the gold phrases and will be expected to provide their system output just for triples.
<ul>
<li>Training data:
<ul>
<li>Same as in the practice phase; no new data will be released</li>
</ul>
</li>
<li>Test data:
<ul>
<li>Part 1: Gold annotated contribution sentences will be released</li>
<li>Part 2: Gold annotated phrases from the contribution sentences</li>
</ul>
</li>
</ul>
</li>
</ol>
While participation is encouraged in all <code>Evaluation Phases and Parts</code>, it is not required. Please see our <span style="color:#5dade2">Terms and Conditions</span> for more information.
<h3>Evaluation Metrics</h3>
The evaluation metrics in the Evaluation Phases 1 and 2 will be the standard Precision, Recall, and F-score measures. 
Details of the evaluation units can be found in our <a href="https://github.com/ncg-task/scoring-program/blob/master/evaluation.py" target="_blank">evaluation script</a> or in our <a href="https://github.com/ncg-task/codalab/blob/master/competition.yaml" target="_blank">Codalab competition configuration yaml file</a>.

<h3>Evaluation Submission Format</h3>

For <code>Evaluation Phase 1: End-to-end pipeline testing phase</code>, the submission will have be organized per the following directory structure:

<pre><code>
    [task-name-folder]/
        ├── [article-counter-folder]/
        │   ├── sentences.txt
        │   └── entities.txt
        │   └── triples/
        │   │   └── research-problem.txt
        │   │   └── model.txt
        │   │   └── ...                         # each article may be annotated by 3 or 6 information units
        │   └── ...                             # repeats for all articles annotated in a release
        └── ...                                 # repeats depending on the number of tasks in the release
</code></pre>

<p>Please see our Github repository <a href="https://github.com/ncg-task/sample-submission" target="_blank">https://github.com/ncg-task/sample-submission</a> for detailed information and for sample system input and output data for each of the <code>Evaluation Phases</code>.</p>
<p>&nbsp;</p>