<h3>Data Use Agreements</h3>
<p>The annotated data is released under the <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0 license</a>.</p>

<h3 style="margin-top: 1em;">Releases of Source Text</h3>
<p>Each scholarly article will be provided in plain text format preprocessed for sentence splitting and tokenization.</p>

<h3 id="Releases-of-Annotations" style="margin-top: 1em;">Releases of Annotations</h3>
<p>The annotations for the task will be released on the <a href="https://github.com/ncg-task" target="_blank">NLPContributionGraph Github account</a>. The following releases are currently available and forthcoming:</p>
<ul>
<li><a href="https://github.com/ncg-task/trial-data" target="_blank">Trial data download</a></li>
<li><a href="https://github.com/ncg-task/training-data" target="_blank">Training data Coming Soon...</a></li>
<li><a href="https://github.com/ncg-task/test-data-phase2-part1" target="_blank">Evaluation Phase 2 Part 1 Contribution Sentences Forthcoming...</a></li>
<li><a href="https://github.com/ncg-task/test-data-phase2-part2" target="_blank">Evaluation Phase 2 Part 2 Phrases Forthcoming...</a></li>
</ul>

<h3 style="margin-top: 1em;">Corpus Annotation</h3>
<p>The NLPContributionGraph data comprises a corpus of NLP scholarly articles. The annotation scheme was devised after a two stage annotation exercise:</p>
<ol>
<li>Pilot Stage: The annotator identified information units, contribution-related sentences, scientific term and predicate phrases, and formed their triples</li>
<li>Adjudication Stage: In the adjudication stage, the guidelines and annotations were revised as seen necessary</li>
</ol>
<p>The pilot stage annotation methodology can be found described in the <a href="https://arxiv.org/abs/2006.12870" target="_blank">arxiv article</a>.</p>

<h3>Data Format</h3>
<p>The NLPContribution data is in the following format. For the training and dev sets, the annotations for each scholarly article are provided in a directory. 
The directory will contain a plain text file which is the article's full text, preprocessed for tokenization and sentence splitting; 
and the annotations will be provided as the following three files: 
<strong>sentences.txt</strong> containing the annotated contribution sentence numbers from the plain text file; 
<strong>entities.txt</strong> containing the sentence number, tab-separated from the start and end token numbers of the annotated phrase in the sentence; 
and a directory <strong>triples/</strong> containing
files with triples of scientific term phrase pairs and a relation cue phrase, where the files are named by the name of
information unit that the triples data represents.</p>

<p>The data releases will have be organized per the following directory structure:

<pre><code>
    [task-name-folder]/
        ├── [article-counter-folder]/
        │   ├── [articlename].pdf            # scholarly article pdf
        │   ├── [articlename]-Grobid-out.txt # plaintext output from the <a href="https://github.com/kermitt2/grobid" target="_blank">Grobid parser</a>
        │   ├── [articlename]-Stanza-out.txt # plaintext preprocessed output from <a href="https://github.com/stanfordnlp/stanza" target="_blank">Stanza</a>
        │   ├── sentences.txt                # contribution sentences in the file
        │   └── entities.txt                 # terms and predicates in the contribution sentences
        │   └── info-units/                  # folder containing information units in JSON format
        │   │   └── research-problem.json    # includes sentences and entities as JSON objects
        │   │   └── model.json                  
        │   │   └── ...                      # each article may be annotated by 3 or 6 information units`
        │   └── triples/
        │   │   └── research-problem.txt
        │   │   └── model.txt
        │   │   └── ...                      # each article may be annotated by 3 or 6 information units
        │   └── ...                          # repeats for all articles annotated in a release
        └── ...                              # repeats depending on the number of tasks in the release
</code></pre>

The data can be downloaded at their individual release Github links above as they are ready.</p>